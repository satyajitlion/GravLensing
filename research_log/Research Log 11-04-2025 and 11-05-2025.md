### The Focus for These Two Days

My focus was on finishing the notes for Lecture 1.2, delving deeper into the mathematical structure of the roto-translation group $\text{SE}(2)$. I worked on understanding its parametric and matrix representations, the concept of group representations, and how these principles translate to the equivariance property in neural networks.

---
### What I was able to accomplish

- Finished detailed notes on the $\text{SE}(2)$ group, documenting its group product and inverse in both parametric and matrix forms.
    
- Clarified the notation from the lecture, confirming that $\theta$ is a scalar angle while $\mathbf{x}$ and $\mathbf{R}_{\theta}$ are vectors and/or matrices.
    
- Documented the core concept of a group representation $\rho$ and the specific mechanics of the left-regular representation $\mathcal{L}_{g}$â€‹.
    
- Recorded the precise mathematical definition of equivariance for a neural network operator $\Phi$.
    
- Began connecting these abstract concepts to the practical implementation of GCNNs, specifically how kernels as functions on the group assign weights to relative positions.

---
### Results

- I now have some foundational understanding of the $\text{SE}(2)$ group and am a little more comfortable switching between its parametric and matrix representations.
    
- The link between group theory and GCNNs is becoming clearer: viewing feature maps and kernels as functions on the group is quite helpful and leads to the definition of group convolution.
    
- I have a slightly better understanding of equivariance: an operator $\Phi$ is equivariant if transforming the input and then applying $\Phi$ yields the same result as applying $\Phi$ first and then transforming the output. This guarantees that the network's internal representations transform predictably.
    
---
### Challenges & Pause Points

- The biggest challenge was the mathematical concepts when watching the lectures as I had to do extensive research every few minutes while watching the lectures to better understand the math (group representations, homomorphisms) and this required slow, careful reading and multiple passes to fully digest.
    
- Understanding the intuition behind the _left-regular representation_ took me a while as well. I needed to solidify why the _inverse_ is used to ensure the representation properly preserves the group structure.
    
- While the math is slightly clearer, visualizing how a continuous group like $\text{SE}(2)$ is implemented in a practical, finite neural network layer is still an abstract concept which is quite weird. I believe to fully grasp what the nuances behind the math, I will have to read more and gain hands on experience on this specific type of network.
---
### Questions & Ideas

- In practice, for a discrete number of images, how is the $\text{SE}(2)$ group implemented and how do we define roto-translations in code? How will the kernel be rotated to match the angle of the image if it was rotated? 
    
- How is the group convolution implemented in code?
---
### Next Steps

1. Review the notes on the left-regular representation to build a stronger intuitive understanding of its structure and purpose.
    
2. Proceed to the next lecture in the series, which likely covers the actual group convolution operation in detail.
    
3. Start looking for a simple, well-documented code implementation of a 2D GCNN (e.g., in PyTorch) to connect these mathematical concepts to practical code structures.
    
4. Revisit the concept of semi-direct products to fully grasp the structure of SE(2).

##### Tags: #GroupEquivariantCNNs #GroupTheory #SE2 #Equivariance #ResearchLog