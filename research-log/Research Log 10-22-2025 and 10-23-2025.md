### The Focus for These Two Days

The primary focus was on advancing the theoretical and computational foundations of the project. This involved:

1. Continuing the study of Group Theory and its application to Group Equivariant Neural Networks.
    
2. Creating the Python script to combine the output arrays from the Amarel batch jobs for analysis.
    
3. Integrating Dr. Keeton's guidance on calculating the lensing potential at image positions into the `Analysis.ipynb` notebook.
***
### What I was able to accomplish

- **Group Theory Study:** Watched instructional videos on Group Theory and completed a detailed proof of Lemma 1.2.2 to solidify understanding of fundamental group properties (uniqueness of identity and inverse elements). This took me quite a bit of time as it has been quite some time since I had taken real analysis or a rigorous math course. Therefore, the mathematical language was a bit tough to immediately understand. There were also small nuances within the Proof for the lemma so I definitely needed time to explore.
    
- **Script Completion:** Successfully wrote and tested the `combine_arrays.py` script. This script efficiently loads, concatenates, and saves the split `valShear`, `valEllip`, and `valBoth` arrays from the `AmarelOutput` directory into a new `CombinedArrays` folder.
    
- **Analysis Implementation:** Implemented Dr. Keeton's Slack recommendation into the `Analysis.ipynb` notebook. Added the code to compute the lensing potential, deflection vectors, and magnification tensors at the solved image positions using `plane.defmag(imgarr)`.
***
### Results

- **Theoretical:** Gained a concrete, practical understanding of group axioms through the proof, which is directly relevant to understanding the symmetries that Group Equivariant Neural Networks are designed to respect.
    
- **Computational:**
    - The data combination script is functional and ready for use once the Amarel batch jobs are complete.
        
    - The analysis notebook is now equipped with the correct method to extract the lensing potential, a critical quantity for further physical analysis and model training.
***
### Challenges & Pause Points

- The group theory proof, while ultimately successful, required careful step-by-step reasoning to ensure each logical deduction was valid. Paused to re-read and confirm the application of Lemma 1.2.1 at multiple points. 
    
- Initially, the distinction between `plane_elpow.defmag()` and `model_elpow.defmag()` was unclear. This was resolved by Dr. Keeton's clarification of how `pygravlens.py` was implemented such that `plane_elpow.defmag()` yielded all of the required values for potential, deflection, and magnification. This confirmed that `plane_elpow.defmag()` is the primary method needed as it returns the potential.
***
### Questions & Ideas

- Now that we can calculate the potential, we could use it as an additional input feature or a physical regularization term for the neural network, potentially (pun not intended) improving its physical consistency.

- The successful array-combination script creates a reliable template. We should generalize this script now to handle future batch jobs for training and test data to avoid rewriting it later (if more than $10^5$ mock lenses will be generated). 
***
### Next Steps

1. Check the batch jobs on Amarel to see if any mistakes were made with the last SLURM implementation ("dumb parallel processing").

2. Rerun Amarel batch jobs again to add on the new potential parameter to the dictionary for all three models.  
    
3. Execute the `combine_arrays.py` script to create the combined datasets (will have to do this again after rerunning Amarel batch jobs post-potential addition).
    
4. In the `Analysis.ipynb` notebook, load the combined data and perform a full analysis including generation of corner plots. 
    
5. Begin structuring the data (images, source parameters, and now calculated potentials) into a format suitable for training a Group Equivariant CNN.

##### Tags:




