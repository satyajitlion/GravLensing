### The Focus for these Two Days

To deepen my understanding of classification algorithms by reviewing Logistic Regression fundamentals and exploring different activation functions used in neural networks. The goal was to connect the concepts from traditional logistic regression to their applications in modern neural networks. 
***
### What I was able to accomplish

<u>tl;dr</u>: Edited [The Math Behind Neural Networks and Keras](https://github.com/satyajitlion/GravLensing/blob/8133860737e765a5e4a30088e2a7ca523b55c9d8/Notes/The%20Math%20Behind%20Neural%20Networks%20and%20Keras.md) and Researched the intricacies behind logistic regression and activation functions, their importance, and how it relates to a neural network that is attemption to learn non-linear patterns between it's inputs and outputs.

- **Completed Logistic Regression Review:** Thoroughly documented the theory behind logistic regression, including its use cases compared to linear regression.
- **Explained the Sigmoid Function:** Detailed how the sigmoid function transforms linear outputs into probabilities between 0 and 1.
- **Documented Cost Function:** Explained the cross-entropy/log loss function for logistic regression and how it handles binary classification.
- **Covered Probability Mapping:** Described how probabilities are converted to discrete classes using a threshold (typically 0.5).
- **Began Activation Function Exploration:** Started researching different types of activation functions, beginning with a deeper analysis of sigmoid's limitations and briefly introducing tanh and ReLU.
***
### Results

- Gained clarity on when to use logistic regression vs. linear regression through concrete examples (house prices vs. ticket classification).
- Understood the mathematical reasoning behind the cross-entropy cost function and how it elegantly handles both classes ($y=1$ and $y=0$) in one formula.
- Recognized that logistic regression can be seen as a single-layer neural network with a sigmoid activation function.
- Identified key limitations of the sigmoid function (vanishing gradients, slow convergence, non-zero-centered outputs) that motivate the use of other activation functions.
***
### Problems

1. Understanding how logistic regression relates to neural networks required connecting the "sigmoid activation" concept across both topics.
2. The derivation and intuition behind the cross-entropy cost function is more complex than MSE for linear regression.
3. Recognizing why such a fundamental function has practical limitations in deep learning was initially counterintuitive.
***
### Questions & Ideas

1. If sigmoid has these limitations, why is it still commonly used in the output layer for binary classification?
2. How do tanh and ReLU specifically address the limitations of sigmoid? What are their own advantages and disadvantages?
***
### Next Steps

1. Learn more about the tanh and ReLU functions, including their mathematical properties, derivatives, and practical applications.
2. Explicitly map how logistic regression concepts translate to neural network layers and activation functions.
3. Maybe code a logistic regression model to apply these concepts hands-on.
4. Research how these binary classification concepts extend to multi-class problems using softmax activation.

##### Tags: #LogisticRegression #ActivationFunctions #Sigmoid #Classification #Foundations #Keras #NeuralNetworks 




