### The Focus for These Two Days

My focus was on building a foundational understanding of Group Equivariant Neural Networks. I started by watching Erik Bekkers' lecture series to learn the core motivations for G-CNNs, the basics of group theory as it applies to deep learning, and the specific mathematical structure of the roto-translation group SE(2).
***
### What I was able to accomplish

- Watched and took detailed notes on Lectures 1.1 and 1.2 of the "Group Equivariant Deep Learning" playlist. This took a while as some of the mathematics was quite unfamiliar and I wasn't comfortable with the math. 
    
- Understood the key benefits of equivariance: information preservation and guaranteed stability under transformations.
    
- Learned about the hierarchical feature construction in G-CNNs (low $\rightarrow$ mid $\rightarrow$ high-level features) using relative angles and displacements.
    
- Reviewed the formal definition of a group and its axioms.
    
- Documented the specific mathematical formulations for two key groups: the translational group (ℝ², +) and the roto-translation group SE(2), including their group product and inverse operations.
***
### Results

- I now understand that equivariance is a powerful inductive bias for any structured data, not just for achieving invariance. It ensures that the network's internal representations transform predictably when the input is transformed.
    
- I also now understand how features are built hierarchically in a G-CNN, from low-level features to high-level features.
    
- I have a clearer understanding of what SE(2) group refers to which is crucial for understanding how group convolutions are performed beyond simple translations.
***
### Challenges & Pause Points

- While I understand the group theory definitions, I still don't really understand, at least intuitively, how the group product for SE(2) works. I understand what it encodes but the math doesn't entirely make sense. I am also confused as to how this would be implemented in a neural network layer. The coupling of rotation and translation is clear mathematically, but the practical implementation is non-trivial.

- The heart example is helpful, but visualizing how these "arrangements at relative angles and displacements" are actually computed in a feature map is not clear yet. 
***
### Questions & Ideas

***
### Next Steps

##### Tags:




