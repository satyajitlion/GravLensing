### The Focus for These Two Days

My focus was on building a foundational understanding of Group Equivariant Neural Networks. I started by watching Erik Bekkers' lecture series to learn the core motivations for G-CNNs, the basics of group theory as it applies to deep learning, and the specific mathematical structure of the roto-translation group SE(2).
***
### What I was able to accomplish

- Watched and took detailed notes on Lectures 1.1 and 1.2 of the "Group Equivariant Deep Learning" playlist. This took a while as some of the mathematics was quite unfamiliar and I wasn't comfortable with the math. 
    
- Understood the key benefits of equivariance: information preservation and guaranteed stability under transformations.
    
- Learned about the hierarchical feature construction in G-CNNs (low $\rightarrow$ mid $\rightarrow$ high-level features) using relative angles and displacements.
    
- Reviewed the formal definition of a group and its axioms.
    
- Documented the specific mathematical formulations for two key groups: the translational group (ℝ², +) and the roto-translation group SE(2), including their group product and inverse operations.
***
### Results

- I now understand that equivariance is a powerful inductive bias for any structured data, not just for achieving invariance. It ensures that the network's internal representations transform predictably when the input is transformed.
    
- I also now understand how features are built hierarchically in a G-CNN, from low-level features to high-level features.
    
- I have a clearer understanding of what SE(2) group refers to which is crucial for understanding how group convolutions are performed beyond simple translations.
***
### Challenges & Pause Points

- While I understand the group theory definitions, I still don't really understand, at least intuitively, how the group product for SE(2) works. I understand what it encodes but the math doesn't entirely make sense. I am also confused as to how this would be implemented in a neural network layer. The coupling of rotation and translation is clear mathematically, but the practical implementation is non-trivial.

- The heart example is helpful, but visualizing how these "arrangements at relative angles and displacements" are actually computed in a feature map is not clear yet.

- Understanding what the _semi-direct product_ (denoted by ⋊) was and how it fit in the definition of an SE(2) group was a major pause point as, initially, it took me some time to research what that symbol even was. In addition to this, it took me some time to understand what it meant in the context of the definition. 
***
### Questions & Ideas

- In a neural network, how do we actually handle the infinite possible rotations and translations? Do we just pick a few rotation angles to work with?
    
- What does the "filter" or "kernel" look like for an SE(2) convolution? Is it a 3D filter that also changes with orientation?
    
- How much more efficient is the weight sharing in a G-CNN? If the group has more elements (like more rotations), do we share the weights more?
***
### Next Steps

1. Keep watching the lectures in the playlist.

2. Implement a script to separate the data I have such that it is ready to be fed into a neural network.

3. Look for a simple, documented implementation of a G-CNN for SE(2) (e.g., in a library like `escnn` or in a research code repository) to bridge the gap between theory and practice.

4. Revisit the concept of semi-direct products to fully grasp the structure of SE(2). 

5. Try to change my  current network code to match a GCNN model? I should probably test my current code for the current dataset to check if an analysis of the dataset can be done given the right implementation.

##### Tags: #GroupEquivariantCNNs #GroupTheory #NeuralNetworks 




