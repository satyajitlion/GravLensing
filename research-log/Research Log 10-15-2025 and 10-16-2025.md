### The Focus for these Two Days

Exploring neural network architectures that maintain performance under translational and rotational transformations of gravitational lensing data. Investigating the mathematical foundations of group theory and its application to equivariant and invariant neural networks for astrophysical data analysis.
***
### What I was able to accomplish

- Successfully set up and launched 10 parallel batch jobs on Amarel for mock lens generation
    
- Studied fundamental concepts of group theory and symmetry operations
    
- Learned about the mathematical distinctions between equivariance,
```math
f(t(x)) = t'(f(x)),
```
  and invariance,
```math
f(t(x)) = f(x).
```

- Explored how these concepts apply to gravitational lensing systems where image positions may be rotated or translated
    
- Continued investigating sequential neural network implementations in Keras and researched more about how the neural network is saved using `model.save()` and `model.load()`. 
	- I found that this saves the model architecture as well as the weights and biases if the model had been trained prior to saving.
	- What I found interesting here is the models can also be saved architecturally (by saving it as a json) and that the weights can be saved separated as well!

***
### Results

- Batch jobs running successfully with proper task ID handling and file organization
    
- Clear understanding that for gravitational lens analysis, invariance is preferred over equivariance for consistent predictions regardless of coordinate transformations (however, maybe equivariance can be used to get invariance such that no transformation of the output takes place)
    
- Recognition that while equivariance preserves transformation relationships, invariance provides unchanging outputs.
    
- Conceptual framework for potentially reversing output transformations when the input transformation is known (as mentioned earlier).
***
### Challenges & Pause Points

- Determining whether to pursue pure invariance or develop transformation-reversal methods for equivariant outputs
    
- Understanding the complex mathematical foundations of group theory and its practical implementation in neural networks
    
- Balancing theoretical learning with practical implementation timelines
    
- Ensuring the neural network architecture will generalize well to real observational data after training on simulations
***
### Questions & Ideas

- Could we implement a hybrid approach: use equivariant networks but include a transformation reversal layer to achieve effective invariance?
    
- How do we mathematically characterize the transformation groups relevant to gravitational lensing (rotation, translation groups)?
    
- Would data augmentation with random rotations/translations during training be sufficient, or do we need architecturally enforced invariance?
    
- Is there existing work in astrophysics applying group-equivariant networks to similar problems?
    
- Could we leverage the known physical symmetries of gravitational lensing equations to inform the network architecture or encode the lensing equation as a metric into a tensor? Should I even consider tensors?
***
### Next Steps

1. Monitor batch job completion and combine results using the prepared script
    
2. Design initial neural network architecture focusing on transformation invariance
    
3. Implement basic sequential network in Keras as baseline
    
4. Research existing equivariant neural network implementations (e.g., group-equivariant CNNs or ENNs further)
    
5. Develop strategy for testing network performance under coordinate transformations?
    
6. Plan data preprocessing pipeline that maintains physical meaningfulness while enabling machine learning

##### Tags: #Amarel #Keras #NeuralNetworks #GroupTheory #ENNs #GroupEquivariantCNNs #Transformation-Invariance #GravitationalLensing




